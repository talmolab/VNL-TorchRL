{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./../dataset/adam_exp_tr.p\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['qpos', 'xpos', 'walker_body_sites', 'offsets', 'names_qpos', 'names_xpos', 'kp_data', 'FTOL', 'ROOT_FTOL', 'LIMB_FTOL', 'ROOT_MAXITER', 'Q_MAXITER', 'M_MAXITER', 'MAXITER', 'LR', 'N_ITERS', 'KEYPOINT_MODEL_PAIRS', 'KEYPOINT_INITIAL_OFFSETS', 'KEYPOINT_COLOR_PAIRS', 'TRUNK_OPTIMIZATION_KEYPOINTS', 'INDIVIDUAL_PART_OPTIMIZATION', 'MANDIBLE_POS', 'M_REG_COEF', 'SITES_TO_REGULARIZE', 'N_SAMPLE_FRAMES', 'RENDER_FPS', 'ARENA_TYPE', 'Z_OFFSET', 'SCALE_FACTOR', 'N_FRAMES_PER_CLIP', 'N_GPUS', 'n_fit_frames', 'LR_INIT', 'LR_END', 'kp_names', 'site_index_map', 'part_names', 'indiv_parts', 'lb', 'ub'])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.3515626 ,  0.03942965,  0.0693734 , ...,  0.23565875,\n",
       "         0.5296247 ,  0.        ],\n",
       "       [ 0.35184425,  0.03966405,  0.07004784, ...,  0.23772177,\n",
       "         0.47441334,  0.        ],\n",
       "       [ 0.35186288,  0.03962352,  0.0700995 , ...,  0.23034938,\n",
       "         0.45249096,  0.        ],\n",
       "       ...,\n",
       "       [-0.16206874,  0.13125147,  0.10722869, ...,  0.07409751,\n",
       "         0.4402747 ,  0.        ],\n",
       "       [-0.16220611,  0.13103767,  0.10731128, ...,  0.04319814,\n",
       "         0.49566904,  0.        ],\n",
       "       [-0.1609122 ,  0.13086325,  0.10761824, ...,  0.02327436,\n",
       "         0.4924209 ,  0.        ]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"qpos\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imitation Learning\n",
    "\n",
    "Use the expert demonstration of the `qpos` and `xpos` generated from the STAC, and use optimization methods that allows our agent to imitate from. However, that we assume we have the (state, action) pairs from the experts, but we only have the demonstration/state of the experts. Basic algorithm such as BC is cannot be applied here just because of we don't have access to the action in the motion capture data, contrasted in previous class project.\n",
    "\n",
    "Link to the [CoMic paper](https://proceedings.mlr.press/v119/hasenclever20a/hasenclever20a.pdf) about how they did the multiple clip imitation learning from mocap.\n",
    "\n",
    "Don't know how far we want to re-invent the wheel in torch-rl for multi-clips imitation. They open source it in the dm_control package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
