{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TorchRL Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.envs import BraxWrapper\n",
    "import brax.envs as brax_envs\n",
    "from Rodent_Env_Brax import Rodent\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "brax_envs.register_environment(\"rodent\", Rodent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BraxWrapper(brax_envs.get_environment(\"rodent\"), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env.set_seed(0)\n",
    "# td = env.reset()\n",
    "# print(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# td[\"action\"] = env.action_spec.rand() # random move\n",
    "# td = env.step(td) # step the env\n",
    "# print(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# td = env.rand_step(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %timeit env.rand_step(td)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking observation connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1260"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(env.observation_spec['observation'].shape).shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking parralel environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchrl.modules import (\n",
    "    ActorValueOperator,\n",
    "    ConvNet,\n",
    "    MLP,\n",
    "    OneHotCategorical,\n",
    "    ProbabilisticActor,\n",
    "    TanhNormal,\n",
    "    ValueOperator,\n",
    "    )\n",
    "\n",
    "from torchrl.envs import (BraxWrapper,\n",
    "                          ParallelEnv,\n",
    "                          EnvCreator,\n",
    "                          TransformedEnv,\n",
    "                          VecNorm,\n",
    "                          RewardSum,\n",
    "                          ExplorationType,\n",
    "                          )\n",
    "\n",
    "import brax.envs as brax_envs\n",
    "\n",
    "from tensordict.nn import TensorDictModule\n",
    "\n",
    "from torchrl.data import CompositeSpec\n",
    "\n",
    "from torchrl.data.tensor_specs import DiscreteBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name=\"rodent\", frame_skip=4, is_test=False):\n",
    "    brax_envs.register_environment(env_name, Rodent)\n",
    "\n",
    "    env = BraxWrapper(brax_envs.get_environment(env_name), \n",
    "                      iterations=6,\n",
    "                      ls_iterations=3)\n",
    "    env.set_seed(0)\n",
    "    env = TransformedEnv(env)\n",
    "    return env\n",
    "\n",
    "def make_parallel_env(env_name, num_envs, device, is_test=False):\n",
    "    env = ParallelEnv(\n",
    "        num_envs,\n",
    "        EnvCreator(lambda: make_env(env_name)),\n",
    "        serial_for_single=True,\n",
    "        device=device,\n",
    "    )\n",
    "    env = TransformedEnv(env)\n",
    "    env.append_transform(VecNorm(in_keys=[\"observation\"]))\n",
    "    env.append_transform(RewardSum())\n",
    "    return env\n",
    "\n",
    "proof_environment = make_parallel_env('rodent', 1, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1260])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proof_environment.observation_spec[\"observation\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.,  ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones(proof_environment.observation_spec[\"observation\"].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking cnn shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kevinb/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/torch/nn/modules/lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "common_cnn = ConvNet(\n",
    "        activation_class=torch.nn.ReLU,\n",
    "        num_cells=[32, 64, 64],\n",
    "        kernel_sizes=[8, 4, 3],\n",
    "        strides=[4, 2, 1],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# common_cnn(torch.ones(proof_environment.observation_spec[\"observation\"].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking mlp shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = proof_environment.observation_spec[\"observation\"].shape\n",
    "in_keys = [\"observation\"]\n",
    "\n",
    "common_mlp = MLP(\n",
    "        in_features=input_shape[-1], #common_cnn_output.shape[-1],\n",
    "        activation_class=torch.nn.ReLU,\n",
    "        activate_last_layer=True,\n",
    "        out_features=512,\n",
    "        num_cells=[],\n",
    "    )\n",
    "\n",
    "common_mlp_output = common_mlp(torch.ones(input_shape))#(common_cnn_output)\n",
    "\n",
    "common_module = TensorDictModule(\n",
    "        module=torch.nn.Sequential(common_mlp),#(common_cnn, common_mlp),\n",
    "        in_keys=in_keys,\n",
    "        out_keys=[\"common_features\"],\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking policy & value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "if isinstance(proof_environment.action_spec.space, DiscreteBox):\n",
    "        num_outputs = proof_environment.action_spec.space.n\n",
    "        distribution_class = OneHotCategorical\n",
    "        distribution_kwargs = {}\n",
    "else:  # is ContinuousBox\n",
    "        num_outputs = proof_environment.action_spec.shape\n",
    "        distribution_class = TanhNormal\n",
    "        distribution_kwargs = {\n",
    "            \"min\": proof_environment.action_spec.space.low,\n",
    "            \"max\": proof_environment.action_spec.space.high,\n",
    "}\n",
    "\n",
    "policy_net = MLP(\n",
    "        in_features=common_mlp_output.shape[-1],\n",
    "        out_features=num_outputs,\n",
    "        activation_class=torch.nn.ReLU,\n",
    "        num_cells=[],\n",
    ")\n",
    "policy_module = TensorDictModule(\n",
    "        module=policy_net,\n",
    "        in_keys=[\"common_features\"],\n",
    "        out_keys=[\"logits\"],\n",
    ")\n",
    "\n",
    "# Add probabilistic sampling of the actions\n",
    "policy_module = ProbabilisticActor(\n",
    "        policy_module,\n",
    "        in_keys=[\"logits\"],\n",
    "        spec=CompositeSpec(action=proof_environment.action_spec),\n",
    "        distribution_class=distribution_class,\n",
    "        distribution_kwargs=distribution_kwargs,\n",
    "        return_log_prob=True,\n",
    "        default_interaction_type=ExplorationType.RANDOM,\n",
    ")\n",
    "\n",
    "# Define another head for the value\n",
    "value_net = MLP(\n",
    "        activation_class=torch.nn.ReLU,\n",
    "        in_features=common_mlp_output.shape[-1],\n",
    "        out_features=1,\n",
    "        num_cells=[],\n",
    ")\n",
    "value_module = ValueOperator(\n",
    "        value_net,\n",
    "        in_keys=[\"common_features\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking `make_ppo_models` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorDict(\n",
       "    fields={\n",
       "        action: Tensor(shape=torch.Size([1, 100, 30]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        done: Tensor(shape=torch.Size([1, 100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "        episode_reward: Tensor(shape=torch.Size([1, 100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        next: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([1, 100, 1]), device=cpu, dtype=torch.bool, is_shared=False),\n",
       "                episode_reward: Tensor(shape=torch.Size([1, 100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                observation: Tensor(shape=torch.Size([1, 100, 1260]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([1, 100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                state: TensorDict(\n",
       "                    fields={\n",
       "                        done: Tensor(shape=torch.Size([1, 100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        metrics: TensorDict(\n",
       "                            fields={\n",
       "                                distance_from_origin: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                forward_reward: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                reward_alive: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                reward_linvel: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                reward_quadctrl: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                x_position: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                x_velocity: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                y_position: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                y_velocity: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                            batch_size=torch.Size([1, 100]),\n",
       "                            device=cpu,\n",
       "                            is_shared=False),\n",
       "                        obs: Tensor(shape=torch.Size([1, 100, 1260]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        pipeline_state: TensorDict(\n",
       "                            fields={\n",
       "                                act: Tensor(shape=torch.Size([1, 100, 30]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                act_dot: Tensor(shape=torch.Size([1, 100, 30]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                actuator_length: Tensor(shape=torch.Size([1, 100, 30]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                actuator_moment: Tensor(shape=torch.Size([1, 100, 30, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                actuator_velocity: Tensor(shape=torch.Size([1, 100, 30]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                cam_xmat: Tensor(shape=torch.Size([1, 100, 7, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                cam_xpos: Tensor(shape=torch.Size([1, 100, 7, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                cdof: Tensor(shape=torch.Size([1, 100, 73, 6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                cdof_dot: Tensor(shape=torch.Size([1, 100, 73, 6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                cinert: Tensor(shape=torch.Size([1, 100, 66, 10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                contact: TensorDict(\n",
       "                                    fields={\n",
       "                                        dist: Tensor(shape=torch.Size([1, 100, 34]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                        elasticity: Tensor(shape=torch.Size([1, 100, 34]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                        frame: Tensor(shape=torch.Size([1, 100, 34, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                        friction: Tensor(shape=torch.Size([1, 100, 34, 5]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                        geom1: Tensor(shape=torch.Size([1, 100, 34]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                                        geom2: Tensor(shape=torch.Size([1, 100, 34]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                                        includemargin: Tensor(shape=torch.Size([1, 100, 34]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                        link_idx: TensorDict(\n",
       "                                            fields={\n",
       "                                                item0: Tensor(shape=torch.Size([1, 100, 34]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                                                item1: Tensor(shape=torch.Size([1, 100, 34]), device=cpu, dtype=torch.int32, is_shared=False)},\n",
       "                                            batch_size=torch.Size([1, 100]),\n",
       "                                            device=cpu,\n",
       "                                            is_shared=False),\n",
       "                                        pos: Tensor(shape=torch.Size([1, 100, 34, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                        solimp: Tensor(shape=torch.Size([1, 100, 34, 5]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                        solref: Tensor(shape=torch.Size([1, 100, 34, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                        solreffriction: Tensor(shape=torch.Size([1, 100, 34, 2]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                    batch_size=torch.Size([1, 100]),\n",
       "                                    device=cpu,\n",
       "                                    is_shared=False),\n",
       "                                crb: Tensor(shape=torch.Size([1, 100, 66, 10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                ctrl: Tensor(shape=torch.Size([1, 100, 30]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                cvel: Tensor(shape=torch.Size([1, 100, 66, 6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                efc_D: Tensor(shape=torch.Size([1, 100, 203]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                efc_J: Tensor(shape=torch.Size([1, 100, 203, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                efc_aref: Tensor(shape=torch.Size([1, 100, 203]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                efc_force: Tensor(shape=torch.Size([1, 100, 203]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                efc_frictionloss: Tensor(shape=torch.Size([1, 100, 203]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                eq_active: Tensor(shape=torch.Size([1, 100, 0]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                                geom_xmat: Tensor(shape=torch.Size([1, 100, 101, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                geom_xpos: Tensor(shape=torch.Size([1, 100, 101, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                q: Tensor(shape=torch.Size([1, 100, 74]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qLD: Tensor(shape=torch.Size([1, 100, 73, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qLDiagInv: Tensor(shape=torch.Size([1, 100, 0]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qM: Tensor(shape=torch.Size([1, 100, 73, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qacc: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qacc_smooth: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qacc_warmstart: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qd: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qfrc_actuator: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qfrc_applied: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qfrc_bias: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qfrc_constraint: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qfrc_inverse: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qfrc_passive: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qfrc_smooth: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qpos: Tensor(shape=torch.Size([1, 100, 74]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                qvel: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                site_xmat: Tensor(shape=torch.Size([1, 100, 21, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                site_xpos: Tensor(shape=torch.Size([1, 100, 21, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                solver_niter: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                                subtree_com: Tensor(shape=torch.Size([1, 100, 66, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                time: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                x: TensorDict(\n",
       "                                    fields={\n",
       "                                        pos: Tensor(shape=torch.Size([1, 100, 65, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                        rot: Tensor(shape=torch.Size([1, 100, 65, 4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                    batch_size=torch.Size([1, 100]),\n",
       "                                    device=cpu,\n",
       "                                    is_shared=False),\n",
       "                                xanchor: Tensor(shape=torch.Size([1, 100, 68, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                xaxis: Tensor(shape=torch.Size([1, 100, 68, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                xd: TensorDict(\n",
       "                                    fields={\n",
       "                                        ang: Tensor(shape=torch.Size([1, 100, 65, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                        vel: Tensor(shape=torch.Size([1, 100, 65, 3]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                                    batch_size=torch.Size([1, 100]),\n",
       "                                    device=cpu,\n",
       "                                    is_shared=False),\n",
       "                                xfrc_applied: Tensor(shape=torch.Size([1, 100, 66, 6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                ximat: Tensor(shape=torch.Size([1, 100, 66, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                xipos: Tensor(shape=torch.Size([1, 100, 66, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                xmat: Tensor(shape=torch.Size([1, 100, 66, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                xpos: Tensor(shape=torch.Size([1, 100, 66, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                xquat: Tensor(shape=torch.Size([1, 100, 66, 4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                            batch_size=torch.Size([1, 100]),\n",
       "                            device=cpu,\n",
       "                            is_shared=False),\n",
       "                        reward: Tensor(shape=torch.Size([1, 100, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                    batch_size=torch.Size([1, 100]),\n",
       "                    device=cpu,\n",
       "                    is_shared=False),\n",
       "                terminated: Tensor(shape=torch.Size([1, 100, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "            batch_size=torch.Size([1, 100]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        observation: Tensor(shape=torch.Size([1, 100, 1260]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "        state: TensorDict(\n",
       "            fields={\n",
       "                done: Tensor(shape=torch.Size([1, 100, 1]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                metrics: TensorDict(\n",
       "                    fields={\n",
       "                        distance_from_origin: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        forward_reward: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        reward_alive: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        reward_linvel: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        reward_quadctrl: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        x_position: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        x_velocity: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        y_position: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        y_velocity: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                    batch_size=torch.Size([1, 100]),\n",
       "                    device=cpu,\n",
       "                    is_shared=False),\n",
       "                obs: Tensor(shape=torch.Size([1, 100, 1260]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                pipeline_state: TensorDict(\n",
       "                    fields={\n",
       "                        act: Tensor(shape=torch.Size([1, 100, 30]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        act_dot: Tensor(shape=torch.Size([1, 100, 30]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        actuator_length: Tensor(shape=torch.Size([1, 100, 30]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        actuator_moment: Tensor(shape=torch.Size([1, 100, 30, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        actuator_velocity: Tensor(shape=torch.Size([1, 100, 30]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        cam_xmat: Tensor(shape=torch.Size([1, 100, 7, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        cam_xpos: Tensor(shape=torch.Size([1, 100, 7, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        cdof: Tensor(shape=torch.Size([1, 100, 73, 6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        cdof_dot: Tensor(shape=torch.Size([1, 100, 73, 6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        cinert: Tensor(shape=torch.Size([1, 100, 66, 10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        contact: TensorDict(\n",
       "                            fields={\n",
       "                                dist: Tensor(shape=torch.Size([1, 100, 34]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                elasticity: Tensor(shape=torch.Size([1, 100, 34]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                frame: Tensor(shape=torch.Size([1, 100, 34, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                friction: Tensor(shape=torch.Size([1, 100, 34, 5]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                geom1: Tensor(shape=torch.Size([1, 100, 34]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                                geom2: Tensor(shape=torch.Size([1, 100, 34]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                                includemargin: Tensor(shape=torch.Size([1, 100, 34]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                link_idx: TensorDict(\n",
       "                                    fields={\n",
       "                                        item0: Tensor(shape=torch.Size([1, 100, 34]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                                        item1: Tensor(shape=torch.Size([1, 100, 34]), device=cpu, dtype=torch.int32, is_shared=False)},\n",
       "                                    batch_size=torch.Size([1, 100]),\n",
       "                                    device=cpu,\n",
       "                                    is_shared=False),\n",
       "                                pos: Tensor(shape=torch.Size([1, 100, 34, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                solimp: Tensor(shape=torch.Size([1, 100, 34, 5]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                solref: Tensor(shape=torch.Size([1, 100, 34, 2]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                solreffriction: Tensor(shape=torch.Size([1, 100, 34, 2]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                            batch_size=torch.Size([1, 100]),\n",
       "                            device=cpu,\n",
       "                            is_shared=False),\n",
       "                        crb: Tensor(shape=torch.Size([1, 100, 66, 10]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        ctrl: Tensor(shape=torch.Size([1, 100, 30]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        cvel: Tensor(shape=torch.Size([1, 100, 66, 6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        efc_D: Tensor(shape=torch.Size([1, 100, 203]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        efc_J: Tensor(shape=torch.Size([1, 100, 203, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        efc_aref: Tensor(shape=torch.Size([1, 100, 203]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        efc_force: Tensor(shape=torch.Size([1, 100, 203]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        efc_frictionloss: Tensor(shape=torch.Size([1, 100, 203]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        eq_active: Tensor(shape=torch.Size([1, 100, 0]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                        geom_xmat: Tensor(shape=torch.Size([1, 100, 101, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        geom_xpos: Tensor(shape=torch.Size([1, 100, 101, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        q: Tensor(shape=torch.Size([1, 100, 74]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qLD: Tensor(shape=torch.Size([1, 100, 73, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qLDiagInv: Tensor(shape=torch.Size([1, 100, 0]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qM: Tensor(shape=torch.Size([1, 100, 73, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qacc: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qacc_smooth: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qacc_warmstart: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qd: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qfrc_actuator: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qfrc_applied: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qfrc_bias: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qfrc_constraint: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qfrc_inverse: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qfrc_passive: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qfrc_smooth: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qpos: Tensor(shape=torch.Size([1, 100, 74]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        qvel: Tensor(shape=torch.Size([1, 100, 73]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        site_xmat: Tensor(shape=torch.Size([1, 100, 21, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        site_xpos: Tensor(shape=torch.Size([1, 100, 21, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        solver_niter: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.int32, is_shared=False),\n",
       "                        subtree_com: Tensor(shape=torch.Size([1, 100, 66, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        time: Tensor(shape=torch.Size([1, 100]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        x: TensorDict(\n",
       "                            fields={\n",
       "                                pos: Tensor(shape=torch.Size([1, 100, 65, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                rot: Tensor(shape=torch.Size([1, 100, 65, 4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                            batch_size=torch.Size([1, 100]),\n",
       "                            device=cpu,\n",
       "                            is_shared=False),\n",
       "                        xanchor: Tensor(shape=torch.Size([1, 100, 68, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        xaxis: Tensor(shape=torch.Size([1, 100, 68, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        xd: TensorDict(\n",
       "                            fields={\n",
       "                                ang: Tensor(shape=torch.Size([1, 100, 65, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                                vel: Tensor(shape=torch.Size([1, 100, 65, 3]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                            batch_size=torch.Size([1, 100]),\n",
       "                            device=cpu,\n",
       "                            is_shared=False),\n",
       "                        xfrc_applied: Tensor(shape=torch.Size([1, 100, 66, 6]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        ximat: Tensor(shape=torch.Size([1, 100, 66, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        xipos: Tensor(shape=torch.Size([1, 100, 66, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        xmat: Tensor(shape=torch.Size([1, 100, 66, 3, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        xpos: Tensor(shape=torch.Size([1, 100, 66, 3]), device=cpu, dtype=torch.float32, is_shared=False),\n",
       "                        xquat: Tensor(shape=torch.Size([1, 100, 66, 4]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "                    batch_size=torch.Size([1, 100]),\n",
       "                    device=cpu,\n",
       "                    is_shared=False),\n",
       "                reward: Tensor(shape=torch.Size([1, 100, 1]), device=cpu, dtype=torch.float32, is_shared=False)},\n",
       "            batch_size=torch.Size([1, 100]),\n",
       "            device=cpu,\n",
       "            is_shared=False),\n",
       "        terminated: Tensor(shape=torch.Size([1, 100, 1]), device=cpu, dtype=torch.bool, is_shared=False)},\n",
       "    batch_size=torch.Size([1, 100]),\n",
       "    device=cpu,\n",
       "    is_shared=False)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "proof_environment.rollout(max_steps=100, break_when_any_done=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_critic = ActorValueOperator(\n",
    "        common_operator=common_module,\n",
    "        policy_operator=policy_module,\n",
    "        value_operator=value_module,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "distribution keywords and tensordict keys indicated by ProbabilisticTensorDictModule.dist_keys must match.Got this error message: \n    TanhNormal.__init__() got an unexpected keyword argument 'logits'\nwith dist_keys=['logits']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/tensordict/nn/probabilistic.py:344\u001b[0m, in \u001b[0;36mProbabilisticTensorDictModule.get_dist\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    343\u001b[0m         dist_kwargs[dist_key] \u001b[38;5;241m=\u001b[39m tensordict\u001b[38;5;241m.\u001b[39mget(td_key)\n\u001b[0;32m--> 344\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdist_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribution_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mTypeError\u001b[0m: TanhNormal.__init__() got an unexpected keyword argument 'logits'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      2\u001b[0m     td \u001b[38;5;241m=\u001b[39m proof_environment\u001b[38;5;241m.\u001b[39mrollout(max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, break_when_any_done\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m     td \u001b[38;5;241m=\u001b[39m \u001b[43mactor_critic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m td\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/tensordict/nn/common.py:289\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(out[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m dest)\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/tensordict/_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/tensordict/nn/utils.py:261\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[0;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    256\u001b[0m     skip_existing()\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mkeys(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m out_keys)\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m out_keys \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m in_keys)\n\u001b[1;32m    259\u001b[0m ):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensordict\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/tensordict/nn/sequence.py:428\u001b[0m, in \u001b[0;36mTensorDictSequential.forward\u001b[0;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs):\n\u001b[1;32m    427\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule:\n\u001b[0;32m--> 428\u001b[0m         tensordict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    431\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorDictSequential does not support keyword arguments other than \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtensordict_out\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or in_keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    432\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/tensordict/nn/sequence.py:409\u001b[0m, in \u001b[0;36mTensorDictSequential._run_module\u001b[0;34m(self, module, tensordict, **kwargs)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_module\u001b[39m(\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    402\u001b[0m     module: TensorDictModule,\n\u001b[1;32m    403\u001b[0m     tensordict: TensorDictBase,\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    405\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_tolerant \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[1;32m    407\u001b[0m         key \u001b[38;5;129;01min\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mkeys(include_nested\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39min_keys\n\u001b[1;32m    408\u001b[0m     ):\n\u001b[0;32m--> 409\u001b[0m         tensordict \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpartial_tolerant \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensordict, LazyStackedTensorDict):\n\u001b[1;32m    411\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m sub_td \u001b[38;5;129;01min\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mtensordicts:\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/tensordict/nn/common.py:289\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(out[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m dest)\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/tensordict/_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/tensordict/nn/utils.py:261\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[0;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    256\u001b[0m     skip_existing()\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mkeys(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m out_keys)\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m out_keys \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m in_keys)\n\u001b[1;32m    259\u001b[0m ):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensordict\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/tensordict/nn/probabilistic.py:575\u001b[0m, in \u001b[0;36mProbabilisticTensorDictSequential.forward\u001b[0;34m(self, tensordict, tensordict_out, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m(auto_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    567\u001b[0m \u001b[38;5;129m@set_skip_existing\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    573\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m TensorDictBase:\n\u001b[1;32m    574\u001b[0m     tensordict_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_dist_params(tensordict, tensordict_out, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodule\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_requires_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_requires_sample\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/tensordict/nn/common.py:289\u001b[0m, in \u001b[0;36mdispatch.__call__.<locals>.wrapper\u001b[0;34m(_self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    287\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(out[key] \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m dest)\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m out\n\u001b[0;32m--> 289\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/tensordict/_contextlib.py:126\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 126\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/tensordict/nn/utils.py:261\u001b[0m, in \u001b[0;36mset_skip_existing.__call__.<locals>.wrapper\u001b[0;34m(_self, tensordict, *args, **kwargs)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    256\u001b[0m     skip_existing()\n\u001b[1;32m    257\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m tensordict\u001b[38;5;241m.\u001b[39mkeys(\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m out_keys)\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28many\u001b[39m(key \u001b[38;5;129;01min\u001b[39;00m out_keys \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m in_keys)\n\u001b[1;32m    259\u001b[0m ):\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tensordict\n\u001b[0;32m--> 261\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_self\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/tensordict/nn/probabilistic.py:388\u001b[0m, in \u001b[0;36mProbabilisticTensorDictModule.forward\u001b[0;34m(self, tensordict, tensordict_out, _requires_sample)\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tensordict_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m     tensordict_out \u001b[38;5;241m=\u001b[39m tensordict\n\u001b[0;32m--> 388\u001b[0m dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_dist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _requires_sample:\n\u001b[1;32m    390\u001b[0m     out_tensors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dist_sample(dist, interaction_type\u001b[38;5;241m=\u001b[39minteraction_type())\n",
      "File \u001b[0;32m~/Desktop/salk/rodent/VNL-TorchRL/.venv/lib/python3.11/site-packages/tensordict/nn/probabilistic.py:347\u001b[0m, in \u001b[0;36mProbabilisticTensorDictModule.get_dist\u001b[0;34m(self, tensordict)\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man unexpected keyword argument\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n\u001b[0;32m--> 347\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    348\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistribution keywords and tensordict keys indicated by ProbabilisticTensorDictModule.dist_keys must match.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    349\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot this error message: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mindent(\u001b[38;5;28mstr\u001b[39m(err),\u001b[38;5;250m \u001b[39m\u001b[38;5;241m4\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mwith dist_keys=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    350\u001b[0m         )\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmissing.*required positional arguments\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(err)):\n\u001b[1;32m    352\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    353\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTensorDict with keys \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtensordict\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match the distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m keywords.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    354\u001b[0m         )\n",
      "\u001b[0;31mTypeError\u001b[0m: distribution keywords and tensordict keys indicated by ProbabilisticTensorDictModule.dist_keys must match.Got this error message: \n    TanhNormal.__init__() got an unexpected keyword argument 'logits'\nwith dist_keys=['logits']"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    td = proof_environment.rollout(max_steps=100, break_when_any_done=False)\n",
    "    td = actor_critic(td)\n",
    "    del td"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare to brax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_env = Rodent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import brax\n",
    "key = jax.random.key(0)\n",
    "state = b_env.reset(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_step = jax.jit(b_env.step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "action = jax.random.uniform(key, shape=state.pipeline_state.ctrl.shape)\n",
    "state = jit_step(state, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.68 ms  434 s per loop (mean  std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "action = jax.random.uniform(key, shape=state.pipeline_state.ctrl.shape)\n",
    "%timeit jit_step(state, action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "env = Rodent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.PRNGKey(0)\n",
    "state = env.reset(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pipeline_state', 'obs', 'reward', 'done', 'metrics', 'info'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'brax.mjx.base.State'>\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "<class 'dict'>\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "for k in state.__dict__.keys():\n",
    "    print(type(getattr(state, k)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'brax.base.Contact'>\n"
     ]
    }
   ],
   "source": [
    "data = state.pipeline_state\n",
    "print(type(data.contact))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "contact = data.contact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n",
      "<class 'tuple'>\n",
      "<class 'jaxlib.xla_extension.ArrayImpl'>\n"
     ]
    }
   ],
   "source": [
    "for k in contact.__dict__.keys():\n",
    "    print(type(getattr(contact, k)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1],      dtype=int32),\n",
       " Array([10, 10, 11, 11, 11, 11, 11, 11, 14, 14, 15, 15, 15, 15, 15, 15, 24,\n",
       "        24, 35, 35, 59, 59, 59, 59, 59, 59, 64, 64, 64, 64, 64, 64, 58, 63],      dtype=int32))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "contact.link_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
